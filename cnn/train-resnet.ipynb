{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fc01b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu102\n",
      "0.6.11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Select 2 available GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import ogb\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.lsc import PCQM4Mv2Dataset\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(torch.__version__)\n",
    "print(timm.__version__)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d6decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to competition dataset directory\n",
    "ROOT = '../dataset'\n",
    "dataset = PCQM4Mv2Dataset(root = ROOT, only_smiles =True)\n",
    "\n",
    "# Convert to Dataframe: smiles + target\n",
    "train = pd.DataFrame({\n",
    "    'smiles': [dataset[i][0] for i in range(len(dataset))],\n",
    "    'target': [dataset[i][1] for i in range(len(dataset))]\n",
    "})\n",
    "train.to_parquet('train.parquet')\n",
    "\n",
    "# Get competition splits\n",
    "split_dict = dataset.get_idx_split()\n",
    "train_idx = split_dict['train'] # numpy array storing indices of training molecules\n",
    "valid_idx = split_dict['valid'] # numpy array storing indices of validation molecules\n",
    "testdev_idx = split_dict['test-dev'] # numpy array storing indices of test-dev molecules\n",
    "testchallenge_idx = split_dict['test-challenge'] # numpy array storing indices of test-challenge molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0855d496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3308311\n",
       "3     147432\n",
       "2     147037\n",
       "1     143840\n",
       "Name: fold, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the fold to be trained. Values ranges from 0 to 3\n",
    "FOLD = 0 # 0, 1, 2, 3\n",
    "\n",
    "#Point to development folds split file\n",
    "splits = torch.load('../new_split_dict.pt')\n",
    "\n",
    "# Load smiles dataframe\n",
    "train = pd.read_parquet('train.parquet')\n",
    "\n",
    "train['path'] = '../images/' + train['id'].map(str) + '.png'\n",
    "\n",
    "train['fold'] = 0\n",
    "train.loc[train['id'].isin(splits[f'valid_{FOLD}']), 'fold'] = 1\n",
    "train.loc[train['id'].isin(testdev_idx), 'fold'] = 2\n",
    "train.loc[train['id'].isin(testchallenge_idx), 'fold'] = 3\n",
    "train['fold'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b176d7f",
   "metadata": {},
   "source": [
    "# Augmentations used for train and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e229a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "def get_augmentations():\n",
    "    \n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225) \n",
    "    train_augmentations = albu.Compose([\n",
    "        albu.ShiftScaleRotate (shift_limit=0.10, scale_limit=0.10, rotate_limit=10, interpolation=1, border_mode=4, p=0.75),\n",
    "        albu.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.70, 1.00), ratio=(0.95, 1.05), p=1.0),\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(0.5),\n",
    "        albu.Normalize(always_apply=True),        \n",
    "        ToTensorV2(p=1.0)\n",
    "    ], p=1.0)\n",
    "    \n",
    "    valid_augmentations = albu.Compose([\n",
    "        albu.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        albu.Normalize(always_apply=True),        \n",
    "        ToTensorV2(p=1.0)\n",
    "    ], p=1.0)   \n",
    "    \n",
    "    return train_augmentations, valid_augmentations\n",
    "\n",
    "train_augs, valid_augs = get_augmentations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca0c66",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6be7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, train_mode=0, transforms=None):\n",
    "        self.train = pd.read_parquet('train.parquet')\n",
    "        self.train['path'] = '../images/' + self.train['id'].map(str) + '.png'\n",
    "\n",
    "        # Folds split file\n",
    "        splits = torch.load('../new_split_dict.pt')\n",
    "        self.train['fold'] = 0\n",
    "        self.train.loc[self.train['id'].isin(splits[f'valid_{FOLD}']), 'fold'] = 1\n",
    "        self.train.loc[self.train['id'].isin(testdev_idx), 'fold'] = 2\n",
    "        self.train.loc[self.train['id'].isin(testchallenge_idx), 'fold'] = 3\n",
    "        \n",
    "        self.train = self.train.loc[self.train['fold']==train_mode].reset_index(drop=True).copy()\n",
    "        print(self.train.shape)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.train_mode = train_mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.train.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.train.iloc[index].path\n",
    "        try:\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            image = np.zeros((384, 384, 3), dtype='uint8')\n",
    "        \n",
    "        if (self.transforms):\n",
    "            image = self.transforms(image=image)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            \"x\": image.float(),\n",
    "            \"y\": torch.tensor(self.train.iloc[index, self.train.columns.str.startswith('target')], dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aa15d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3308311, 5)\n"
     ]
    }
   ],
   "source": [
    "# Check data loader\n",
    "dataset = ImageDataset(train_mode=0, transforms=train_augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e07701ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([3, 224, 224]), torch.Size([1]), tensor([6.1743]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dataset[1000]\n",
    "len(x), x['x'].shape, x['y'].shape, x['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df9173f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd5dfa44a60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = x['x'].cpu().numpy()\n",
    "img = np.moveaxis(img, 0, 2)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323d15f",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b261ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, model_name='resnet34', pretrained=True):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.cnn.fc = nn.Linear(512, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.relu(self.cnn(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd875d0",
   "metadata": {},
   "source": [
    "# Training step 1:\n",
    "- LR: 4e-4\n",
    "- Image size: 224\n",
    "- epochs: 12\n",
    "- batch size: 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d58e41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir weights\n",
    "\n",
    "n_epochs = 12\n",
    "batch_size = 1024\n",
    "LR = 4e-4\n",
    "\n",
    "# Load training data loader\n",
    "train_dataset = ImageDataset(train_mode=0, transforms=train_augs)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "# Load validation data loader\n",
    "valid_dataset = ImageDataset(train_mode=1, transforms=valid_augs)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "# Create model\n",
    "model = Model(model_name='resnet34', pretrained=True).to('cuda')\n",
    "# Train using DP, 2x GPUs\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# LR optimizer + scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0 = len(train_loader),\n",
    "    T_mult = 1,\n",
    "    eta_min = 1e-7,\n",
    "    last_epoch= -1,\n",
    ")\n",
    "\n",
    "# Load validation Labels\n",
    "VALID = train.loc[train.fold==1].reset_index(drop=True)\n",
    "\n",
    "best_mae = 1.\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    with tqdm(train_loader) as pbar:\n",
    "        for batch, data in enumerate(pbar):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Casts operations to mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(data['x'].to('cuda'))\n",
    "                loss = criterion(data['y'].to('cuda'), output)\n",
    "\n",
    "            # Scales the loss, and calls backward()\n",
    "            # to create scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Unscales gradients and calls\n",
    "            # or skips optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            # Updates the scale for next iteration\n",
    "            scaler.update()\n",
    "            \n",
    "            scheduler.step(batch)\n",
    "\n",
    "            loss = loss.detach().cpu().numpy()\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            pbar.set_postfix_str(f\"{lr:.6f} train_loss: {loss:.4f}({best_mae:.4f})\")\n",
    "\n",
    "        # Run Validation loop\n",
    "        YPRED = []\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(valid_loader):\n",
    "                ypred = model(data['x'].to('cuda'))\n",
    "                YPRED.append(ypred.cpu().numpy())\n",
    "        YPRED = np.concatenate(YPRED)[:,0]\n",
    "        VALID['pred'] = YPRED\n",
    "        mae = (VALID['target'] - VALID['pred']).abs().mean()\n",
    "\n",
    "        # Update best weight only\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            print(f\"New best: {epoch} {mae}\")\n",
    "            torch.save(model.module.state_dict(), f\"weights/model_resnet34-{FOLD}-{batch_size}-{IMG_SIZE}-v1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79855fb",
   "metadata": {},
   "source": [
    "# Training step 2:\n",
    "- LR: 2e-4\n",
    "- Image size: 224\n",
    "- epochs: 12\n",
    "- batch size: 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba6d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR/2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0 = len(train_loader),\n",
    "    T_mult = 1,\n",
    "    eta_min = 1e-7,\n",
    "    last_epoch= -1,\n",
    ")\n",
    "\n",
    "n_epochs = 12\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm(train_loader) as pbar:\n",
    "        for batch, data in enumerate(pbar):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Casts operations to mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(data['x'].to('cuda'))\n",
    "                loss = criterion(data['y'].to('cuda'), output)\n",
    "\n",
    "            # Scales the loss, and calls backward()\n",
    "            # to create scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Unscales gradients and calls\n",
    "            # or skips optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            # Updates the scale for next iteration\n",
    "            scaler.update()\n",
    "            \n",
    "            scheduler.step(batch)\n",
    "\n",
    "            loss = loss.detach().cpu().numpy()\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            pbar.set_postfix_str(f\"{lr:.6f} train_loss: {loss:.4f}({best_mae:.4f})\")\n",
    "\n",
    "    gc.collect()\n",
    "    # Validation\n",
    "    YPRED = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(valid_loader):\n",
    "            ypred = model(data['x'].to('cuda'))\n",
    "            YPRED.append(ypred.cpu().numpy())\n",
    "    YPRED = np.concatenate(YPRED)[:,0]\n",
    "    VALID['pred'] = YPRED\n",
    "    mae = (VALID['target'] - VALID['pred']).abs().mean()\n",
    "\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        print(f\"New best: {epoch} {mae}\")\n",
    "        torch.save(model.module.state_dict(), f\"weights/model_resnet34-{FOLD}-{batch_size}-{IMG_SIZE}-v1.pt\")\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a3a38",
   "metadata": {},
   "source": [
    "# Training step 3:  \n",
    "- LR: 5e-5\n",
    "- Image size: 288\n",
    "- epochs: 6\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c72db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224+64\n",
    "\n",
    "# Remove Vertical FLip from training\n",
    "def get_augmentations():\n",
    "    \n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225) \n",
    "    \n",
    "    train_augmentations = albu.Compose([\n",
    "        albu.ShiftScaleRotate (shift_limit=0.05, scale_limit=0.05, rotate_limit=5, interpolation=1, border_mode=4, p=0.75),\n",
    "        albu.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.90, 1.00), ratio=(0.95, 1.05), p=1.0),\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        #albu.VerticalFlip(0.5),\n",
    "        albu.Normalize(always_apply=True),        \n",
    "        ToTensorV2(p=1.0)\n",
    "    ], p=1.0)\n",
    "    \n",
    "    valid_augmentations = albu.Compose([\n",
    "        albu.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        albu.Normalize(always_apply=True),        \n",
    "        ToTensorV2(p=1.0)\n",
    "    ], p=1.0)   \n",
    "    \n",
    "    return train_augmentations, valid_augmentations\n",
    "\n",
    "train_augs, valid_augs = get_augmentations()\n",
    "\n",
    "n_epochs = 6\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = ImageDataset(train_mode=0, transforms=train_augs)\n",
    "valid_dataset = ImageDataset(train_mode=1, transforms=valid_augs)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR/4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0 = len(train_loader),\n",
    "    T_mult = 1,\n",
    "    eta_min = 1e-7,\n",
    "    last_epoch= -1,\n",
    ")\n",
    "\n",
    "best_mae = 1.0\n",
    "criterion = nn.L1Loss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm(train_loader) as pbar:\n",
    "        for batch, data in enumerate(pbar):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Casts operations to mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(data['x'].to('cuda'))\n",
    "                loss = criterion(data['y'].to('cuda'), output)\n",
    "\n",
    "            # Scales the loss, and calls backward()\n",
    "            # to create scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Unscales gradients and calls\n",
    "            # or skips optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            # Updates the scale for next iteration\n",
    "            scaler.update()\n",
    "            \n",
    "            scheduler.step(batch)\n",
    "\n",
    "            loss = loss.detach().cpu().numpy()\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            pbar.set_postfix_str(f\"{lr:.6f} train_loss: {loss:.4f}({best_mae:.4f})\")\n",
    "\n",
    "    gc.collect()\n",
    "    # Validation\n",
    "    YPRED = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(valid_loader):\n",
    "            ypred = model(data['x'].to('cuda'))\n",
    "            YPRED.append(ypred.cpu().numpy())\n",
    "    YPRED = np.concatenate(YPRED)[:,0]\n",
    "    VALID['pred'] = YPRED\n",
    "    mae = (VALID['target'] - VALID['pred']).abs().mean()\n",
    "\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        print(f\"New best: {epoch} {mae}\")\n",
    "        torch.save(model.module.state_dict(), f\"weights/model_resnet34-{FOLD}-{batch_size}-{IMG_SIZE}-v1.pt\")\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd48c2",
   "metadata": {},
   "source": [
    "# Training step 4:  \n",
    "- LR: 2.5e-5\n",
    "- Image size: 352\n",
    "- epochs: 12\n",
    "- batch size: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23a52d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224+64+64\n",
    "\n",
    "# Remove shift scale rotate + Vertical flip \n",
    "def get_augmentations():\n",
    "    \n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225) \n",
    "    \n",
    "    train_augmentations = albu.Compose([\n",
    "        #albu.ShiftScaleRotate (shift_limit=0.05, scale_limit=0.05, rotate_limit=5, interpolation=1, border_mode=4, p=0.75),\n",
    "        albu.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.90, 1.00), ratio=(0.95, 1.05), p=1.0),\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        #albu.VerticalFlip(0.5),\n",
    "        albu.Normalize(always_apply=True),        \n",
    "        ToTensorV2(p=1.0)\n",
    "    ], p=1.0)\n",
    "    \n",
    "    valid_augmentations = albu.Compose([\n",
    "        albu.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        albu.Normalize(always_apply=True),        \n",
    "        ToTensorV2(p=1.0)\n",
    "    ], p=1.0)   \n",
    "    \n",
    "    return train_augmentations, valid_augmentations\n",
    "\n",
    "train_augs, valid_augs = get_augmentations()\n",
    "\n",
    "n_epochs = 12\n",
    "batch_size = 512\n",
    "LR = 2e-4\n",
    "\n",
    "train_dataset = ImageDataset(train_mode=0, transforms=train_augs)\n",
    "valid_dataset = ImageDataset(train_mode=1, transforms=valid_augs)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR/8)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0 = len(train_loader),\n",
    "    T_mult = 1,\n",
    "    eta_min = 1e-7,\n",
    "    last_epoch= -1,\n",
    ")\n",
    "\n",
    "best_mae = 1.0\n",
    "criterion = nn.L1Loss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm(train_loader) as pbar:\n",
    "        for batch, data in enumerate(pbar):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Casts operations to mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(data['x'].to('cuda'))\n",
    "                loss = criterion(data['y'].to('cuda'), output)\n",
    "\n",
    "            # Scales the loss, and calls backward()\n",
    "            # to create scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Unscales gradients and calls\n",
    "            # or skips optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            # Updates the scale for next iteration\n",
    "            scaler.update()\n",
    "            \n",
    "            scheduler.step(batch)\n",
    "\n",
    "            loss = loss.detach().cpu().numpy()\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            pbar.set_postfix_str(f\"{lr:.6f} train_loss: {loss:.4f}({best_mae:.4f})\")\n",
    "\n",
    "    gc.collect()\n",
    "    # Validation\n",
    "    YPRED = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(valid_loader):\n",
    "            ypred = model(data['x'].to('cuda'))\n",
    "            YPRED.append(ypred.cpu().numpy())\n",
    "    YPRED = np.concatenate(YPRED)[:,0]\n",
    "    VALID['pred'] = YPRED\n",
    "    mae = (VALID['target'] - VALID['pred']).abs().mean()\n",
    "\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        print(f\"New best: {epoch} {mae}\")\n",
    "        torch.save(model.module.state_dict(), f\"weights/model_resnet34-{FOLD}-{batch_size}-{IMG_SIZE}-v1.pt\")\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a204ead",
   "metadata": {},
   "source": [
    "# Training End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba3e82",
   "metadata": {},
   "source": [
    "# Reload best epoch weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf4b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(model_name='resnet34', pretrained=True).to('cuda')\n",
    "model.load_state_dict(torch.load(f\"weights/model_resnet34-{FOLD}-512-352-v1.pt\"))\n",
    "model = nn.DataParallel(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fcd8f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143840, 5)\n",
      "(147037, 5)\n",
      "(147432, 5)\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 224+64+64\n",
    "\n",
    "valid_augmentations1 = albu.Compose([\n",
    "    albu.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    albu.Normalize(always_apply=True),        \n",
    "    ToTensorV2(p=1.0)\n",
    "], p=1.0)   \n",
    "\n",
    "batch_size = 512\n",
    "valid_dataset = ImageDataset(train_mode=1, transforms=valid_augmentations1)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "test1_dataset = ImageDataset(train_mode=2, transforms=valid_augmentations1)\n",
    "test1_loader = torch.utils.data.DataLoader(test1_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "test2_dataset = ImageDataset(train_mode=3, transforms=valid_augmentations1)\n",
    "test2_loader = torch.utils.data.DataLoader(test2_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef02ca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1124/1124 [05:08<00:00,  3.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(143840,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YPRED = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(valid_loader):\n",
    "        ypred = model(data['x'].to('cuda'))\n",
    "        YPRED.append(ypred.cpu().numpy())\n",
    "yvalid = np.concatenate(YPRED)[:,0]\n",
    "yvalid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c83a2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09870176142906208"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALID['prediction'] = yvalid\n",
    "(VALID['target'] - VALID['prediction']).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9839d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1149/1149 [05:13<00:00,  3.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(147037,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YPRED = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test1_loader):\n",
    "        ypred = model(data['x'].to('cuda'))\n",
    "        YPRED.append(ypred.cpu().numpy())\n",
    "ytest1 = np.concatenate(YPRED)[:,0]\n",
    "ytest1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71b0b55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [05:12<00:00,  3.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(147432,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YPRED = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test2_loader):\n",
    "        ypred = model(data['x'].to('cuda'))\n",
    "        YPRED.append(ypred.cpu().numpy())\n",
    "ytest2 = np.concatenate(YPRED)[:,0]\n",
    "ytest2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d79082e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((143840, 6), (147037, 6), (147432, 6))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "train['path'] = '../images/' + train['id'].map(str) + '.png'\n",
    "\n",
    "splits = torch.load('../new_split_dict.pt')\n",
    "train['fold'] = 0\n",
    "train.loc[train['id'].isin(splits[f'valid_{FOLD}']), 'fold'] = 1\n",
    "train.loc[train['id'].isin(testdev_idx), 'fold'] = 2\n",
    "train.loc[train['id'].isin(testchallenge_idx), 'fold'] = 3\n",
    "\n",
    "VALID = train.loc[train['fold']==1].reset_index(drop=True)\n",
    "TEST1 = train.loc[train['fold']==2].reset_index(drop=True)\n",
    "TEST2 = train.loc[train['fold']==3].reset_index(drop=True)\n",
    "\n",
    "VALID['prediction'] = yvalid\n",
    "TEST1['prediction'] = ytest1\n",
    "TEST2['prediction'] = ytest2\n",
    "\n",
    "VALID.shape, TEST1.shape, TEST2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3713c60b",
   "metadata": {},
   "source": [
    "# Validation MAE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3dcba838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09870176142906208"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(VALID['target'] - VALID['prediction']).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33c92870",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir predictions\n",
    "!mkdir predictions/fold0\n",
    "!mkdir predictions/fold1\n",
    "!mkdir predictions/fold2\n",
    "!mkdir predictions/fold3\n",
    "\n",
    "np.save(f'predictions/fold{FOLD}/valid.npy', VALID['prediction'].values)\n",
    "np.save(f'predictions/fold{FOLD}/devchallenge.npy', TEST1['prediction'].values)\n",
    "np.save(f'predictions/fold{FOLD}/testchallenge.npy', TEST2['prediction'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ccc444",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30cba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
